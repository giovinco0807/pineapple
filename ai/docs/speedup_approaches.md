# FL RL 高速化アプローチ比較

## 現状の問題

- **全探索アルゴリズム**: 毎ステップで全配置パターンをシミュレーション
- **計算量**: O(3^n) で n = 残りカード数
- **結果**: 訓練速度が非常に遅い

---

## アプローチ比較

| アプローチ | 速度 | バースト率 | FL Stay学習 | 実装難易度 |
|-----------|------|-----------|-------------|-----------|
| A. 現状維持 | ⭐ | 0% | ○ | 完了済み |
| B. ソルバー事前計算 | ⭐⭐⭐⭐⭐ | 0% | △ | 中 |
| C. 高速ヒューリスティック | ⭐⭐⭐⭐ | ~5% | ○ | 低 |
| D. 事前計算+キャッシュ | ⭐⭐⭐ | 0% | ○ | 高 |

---

## A. 現状維持（全探索）

```
action_masks() {
    for 各アクション:
        全配置パターンをシミュレーション
        バースト確認
}
```

**メリット**: 確実にバースト0%
**デメリット**: 非常に遅い

---

## B. ソルバー事前計算

```
reset() {
    最適配置 = solver.solve(手札)  # 1回だけ
    許可アクション = 最適配置のパス
}

action_masks() {
    return 許可アクション  # O(1)
}
```

**メリット**: 超高速、バースト0%
**デメリット**: RLが探索する意味がない（ソルバーの解をなぞるだけ）

---

## C. 高速ヒューリスティック

```
action_masks() {
    for 各アクション:
        簡易チェック（O(1)）:
          - middle完成 + bottom完成 → 比較
          - 全完成 → optimize_board
}
```

**メリット**: 高速
**デメリット**: バースト率が5%程度残る

---

## D. 事前計算+キャッシュ

```
reset() {
    全有効配置を列挙しキャッシュ
}

action_masks() {
    キャッシュから現在状態に合致するものを返す
}
```

**メリット**: 高速かつバースト0%
**デメリット**: メモリ使用量大、実装複雑

---

## 推奨

**短期（今すぐ）**: A. 現状維持で長時間訓練
**長期（高速化）**: B. ソルバー事前計算で模倣学習

ソルバー解を模倣させるなら、RLではなく**教師あり学習（Behavior Cloning）**の方が適切かもしれない。
