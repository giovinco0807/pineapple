# OFC Pineapple AI プロジェクト概要

## 1. ゲームルール

### 1.1 基本ルール
Open Face Chinese Poker (OFC) Pineappleは2-3人用のポーカーバリアント。

**ボード構成:**
```
Top:    3枚
Middle: 5枚
Bottom: 5枚
合計:   13枚
```

**勝利条件:**
- Bottom ≥ Middle ≥ Top の強さ順（バストしない）
- 各列で相手に勝てばスコア獲得
- ロイヤリティ（ボーナス）を最大化

### 1.2 Pineappleルール
- 初回: 5枚配布、全て配置
- 以降: 3枚配布、2枚配置、1枚捨て
- バストするとすべてのロイヤリティ無効

### 1.3 Fantasyland（FL）

**エントリー条件:**
- Top: QQ以上 → 14枚
- Top: KK → 15枚
- Top: AA → 16枚
- Top: トリップス → 17枚

**FL中のルール:**
- 全カードを一度に受け取る（14-17枚）
- 13枚を配置、残りを捨て
- 相手に見えない状態でプレイ

**FL Stay条件（継続）:**
- Bottom: Quads または Straight Flush
- Top: Trips

### 1.4 ロイヤリティ一覧

| Top | 点数 |
|-----|------|
| 66 | 1 |
| 77-AA | 2-9 |
| Trips | 10-22 |

| Middle | 点数 |
|--------|------|
| Trips | 2 |
| Straight | 4 |
| Flush | 8 |
| Full House | 12 |
| Quads | 20 |
| Straight Flush | 30 |
| Royal Flush | 50 |

| Bottom | 点数 |
|--------|------|
| Straight | 2 |
| Flush | 4 |
| Full House | 6 |
| Quads | 10 |
| Straight Flush | 15 |
| Royal Flush | 25 |

---

## 2. 現在の実装

### 2.1 ソルバー (`ai/fl_solver.py`)

**2種類のアルゴリズム:**

| 方法 | 速度 | 正確性 |
|------|------|--------|
| Beam Search | 0.05秒 (beam=100) | ~60% |
| Exhaustive Search | 90秒 | 100% |

**Beam Search の問題:**
- beam_width=100: 38%のハンドで最適解を逃す
- beam_width=10000: 未検証（約3秒/ハンド）
- beam_width=100000: 未検証（約41秒/ハンド）

### 2.2 RL環境 (`ai/fl_rl_env.py`)

**観測空間:**
- 各カード: 18次元（ランク13 + スート4 + ジョーカー1）
- 配置情報: 4次元（Top/Middle/Bottom/Discard）
- 合計: 17 × 22 = 374次元

**行動空間:**
- 17カード × 4位置 = 68アクション

**報酬設計:**
- ロイヤリティ合計
- FL Stay ボーナス: +30点
- バースト ペナルティ: -100点

### 2.3 訓練済みモデル

| モデル | 枚数 | バースト率 | FL Stay率 |
|--------|------|-----------|-----------|
| fl_nobust_test | 14 | 0% | 0% |
| fl_solver_fast | 14 | 0% | ~20% |
| fl_17cards | 17 | 0% | 49% |

---

## 3. ロードマップ

### Phase 1: Fantasyland AI ← 現在

#### 完了
- [x] ソルバー実装（Beam/Exhaustive）
- [x] RL環境構築
- [x] バースト防止（Action Masking）
- [x] 基本訓練

#### 進行中
- [ ] Beam Search vs Exhaustive 正確性検証
- [ ] ジョーカー1-2枚のデータ生成
- [ ] Behavior Cloning実装

#### 未着手
- [ ] EVテーブル作成
- [ ] 全枚数（14-17）統合モデル

### Phase 2: 通常プレイAI

- [ ] 1手ずつの配置環境
- [ ] 相手モデリング
- [ ] FL EV を報酬に組み込み

### Phase 3: 統合

- [ ] 通常→FL→通常の遷移
- [ ] エンドツーエンド評価
- [ ] 高速化（WebAssembly）

---

## 4. 現在の懸念点

### 4.1 ソルバーの正確性 vs 速度

**問題:**
- Exhaustive: 正確だが遅すぎる（90秒/ハンド）
- Beam Search: 速いが最適解を逃す（~40%）

**影響:**
- Behavior Cloningの教師データの品質に直結
- 訓練時間とデータ生成時間のトレードオフ

**対策案:**
1. 大きなbeam_width（100000）で妥協点を探す
2. 既存のexhaustiveデータ（3万ハンド）を活用
3. ジョーカーありは別途生成

### 4.2 ジョーカー対応

**問題:**
- 既存データはジョーカー0枚のみ
- ジョーカー1-2枚のデータがない

**影響:**
- ジョーカーありのハンドで性能低下の可能性

**対策:**
- サーバーでジョーカーありデータを生成（数日）

### 4.3 FL Stay率の低さ

**問題:**
- 多くのハンドでFL Stayが不可能
- 報酬が疎になりがち

**影響:**
- 訓練効率が悪い
- RLが正しく学習しにくい

**対策:**
- 中間報酬（FL Stay可能性スコア）を追加
- ヒューリスティック報酬シェーピング

### 4.4 計算リソース

**問題:**
- ローカルPCでは訓練が遅い
- サーバー使用にコストがかかる

**対策:**
- Behavior Cloning（ソルバー不要で高速）
- データ生成はサーバー、訓練はローカル

---

## 5. データファイル

| ファイル | ハンド数 | ジョーカー | 用途 |
|----------|---------|-----------|------|
| fl_joker0_v2.jsonl | 13,067 | 0枚 | Behavior Cloning |
| fl_all.jsonl | 16,081 | 混合? | 確認必要 |
| fl_joker0.jsonl | 6,056 | 0枚 | 予備 |

---

## 6. 次のアクション

1. **beam_width=100000の正確性検証**（進行中）
2. **Behavior Cloning実装**（既存データで）
3. **ジョーカーありデータ生成**（サーバーで並行）
4. **EVテーブル計算**（訓練後）
